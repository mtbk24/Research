{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show a simple example of perfroming linear regression using Maximum Likelihood Estimation. \n",
    "\n",
    "Some of the useful Resources we used can be found here:\n",
    "* http://jekel.me/2016/Maximum-Likelihood-Linear-Regression/\n",
    "* http://suriyadeepan.github.io/2017-01-22-mle-linear-regression/\n",
    "* https://arxiv.org/pdf/1008.4686.pdf\n",
    "    \n",
    "    The last one is a research paper: \n",
    "    Data analysis recipes: Fitting a model to data.  \n",
    "      Authors: David W. Hogg, Jo Bovy, Dustin Lang (2010)\n",
    "        See Equations 9-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "from numpy import std, exp, log, log10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EXAMPLE 1\n",
    "This example uses two separate functions to achieve the calculation. This is a good idea to keep the concepts separate, since you can use the LogLikelihood function with many models, not just a linear model. \n",
    "\n",
    "This function also uses the dot product to handle the errors: np.dot(error.T, error). This becomes problematic if we don't pass the function reasonable starting parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first example we show uses two separate functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(estimate, true, n):\n",
    "    '''\n",
    "    LogLikelihood(estimate, true, n)\n",
    "    -- this function calculates the log-likelihood. To maximize \n",
    "        the likelihood you minimize: -1 * max(likelihood)\n",
    "\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    \n",
    "    estimate: float, is the estimate for y based on the model. \n",
    "    true:     float, is the y-data value. \n",
    "    n:        float, length of the estimate. \n",
    "    \n",
    "    RETURNS:\n",
    "    ----------\n",
    "    returns the log-likelihood. \n",
    "    \n",
    "    \n",
    "    NOTES:\n",
    "    ----------\n",
    "    The error is the true - estimate, or ydata - ymodel. These are also known \n",
    "    as the residuals of the fit; the distances between the actual data point \n",
    "    along the y-axis and the model. Errors/residuals are along y-axis only\n",
    "    meaning they are the vertical offsets and not perpendicular offsets. \n",
    "    Vertical offsets are the most common in linear regression. \n",
    "    \n",
    "    The likelihood function is set up for chi-squared as the statistic to minimize. \n",
    "        \n",
    "    It is in likelihood format:\n",
    "    L = ((1./(2*pi* (sig^2)))^(n/2)) * exp(- ( np.dot(error.T, error)/(2 * (sig^2))))\n",
    "\n",
    "    In log-likelihood format:\n",
    "    LogLike = -(n/2)*log(2*pi*(sig^2)) - (1/(2*(sig^2))) * np.dot(error.T, error)\n",
    "    \n",
    "    L is called the likelihood funtion. LogLike is the log of the likelihood function;\n",
    "    thus, LogLike = log(L).\n",
    "    \n",
    "    error is the error matrix:\n",
    "    (Y - X * theta).T  (Y-X * theta)\n",
    "    \n",
    "    .T stands for the transpose \n",
    "    \n",
    "    '''\n",
    "    error       = true - estimate   # ydata - yModel\n",
    "    sigma       = std(error)        # residual\n",
    "    print(sigma)\n",
    "    L = ((1.0/(2.0*pi*sigma*sigma))**(n/2.)) * exp(-1*((np.dot(error.T, error))/(2.*sigma*sigma)))\n",
    "    return log(L) # log-likelihood.\n",
    "\n",
    "    \n",
    "def line(parameters):\n",
    "    '''\n",
    "    line(parameters)\n",
    "    -- Linear model. This is the function that will be minimized \n",
    "        by the LogLikelihood function. \n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    parameters: list, [m, b], where m and b are the slope and y-intercept. \n",
    "        \n",
    "    '''\n",
    "    m,b       = parameters # m:slope, b:yintercept\n",
    "    yModel    = m * x + b  # estimate of y based on model. \n",
    "    f         = LogLikelihood(yModel, y, len(yModel))\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(estimate, true, n, sigma):\n",
    "    error = true - estimate   # ydata - yModel, aka residual\n",
    "    f = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return f\n",
    "    \n",
    "def line(parameters):\n",
    "    m,b,sigma = parameters # m:slope, b:yintercept\n",
    "    yModel    = m * x + b  # estimate of y based on model. \n",
    "    f         = LogLikelihood(yModel, y, len(yModel), sigma)\n",
    "    return (-1*f)\n",
    "\n",
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b, lnf = theta\n",
    "    model = m * x + b\n",
    "    inv_sigma2 = 1.0/(yerr**2 + model**2*np.exp(2*lnf))\n",
    "    return -0.5*(np.sum((y-model)**2*inv_sigma2 - np.log(inv_sigma2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to setup the LogLike function is to bring the log into the L function. If we do that, we change L to f, since it's no longer the likelihood, but the log-likelikelihood. \n",
    "\n",
    "    f = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return f\n",
    "Notice how we handle the errors differently in this one. You still get the same results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ IN DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log our data because this particular linear relationship, the Amati Correlation equation, is often presented in a form found by logged x- and y-axis data. X-axis (logged Eiso), Y-axis (logged Epeak). We also divide Eiso energies by 1E52 so our y-intercept is inline with most publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('/Users/KimiZ/GRBs2/Sample/AmatiDataSample.txt', sep=',', header=0)\n",
    "x     = log10(data.eiso/(1.0E52))\n",
    "y     = log10(data.epeakRest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.220564660689\n",
      "0.220564660048\n",
      "0.220564660689\n",
      "0.644496485464\n",
      "0.644496493192\n",
      "0.644496485464\n",
      "0.224441058552\n",
      "0.224441060198\n",
      "0.224441058552\n",
      "0.220243803733\n",
      "0.220243804196\n",
      "0.220243803733\n",
      "0.220163944057\n",
      "0.220163944463\n",
      "0.220163944057\n",
      "0.219894459343\n",
      "0.21989445934\n",
      "0.219894459343\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -14.637374487654853\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ -1.81188398e-05,  -1.13686838e-05])\n",
       "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 24\n",
       "      nit: 5\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 0.52089876,  2.05148951])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial input guess of 0.5 for slope and 2 for y-intercept.\n",
    "res = minimize(line, np.array([0.5, 2]), method='L-BFGS-B')\n",
    "res\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.451106704495\n",
      "0.451106711673\n",
      "0.451106704495\n",
      "1.07689185281\n",
      "1.07689186086\n",
      "1.07689185281\n",
      "0.650436032598\n",
      "0.650436040335\n",
      "0.650436032598\n",
      "0.614767786556\n",
      "0.614767794233\n",
      "0.614767786556\n",
      "0.475787803612\n",
      "0.475787810903\n",
      "0.475787803612\n",
      "0.268333869846\n",
      "0.268333874557\n",
      "0.268333869846\n",
      "0.387980808061\n",
      "0.387980814835\n",
      "0.387980808061\n",
      "0.307083147033\n",
      "0.307083141294\n",
      "0.307083147033\n",
      "0.223991640153\n",
      "0.223991641718\n",
      "0.223991640153\n",
      "7.72679840429\n",
      "7.72679839608\n",
      "7.72679840429\n",
      "1.75182417684\n",
      "1.75182416868\n",
      "1.75182417684\n",
      "0.345430887631\n",
      "0.34543088129\n",
      "0.345430887631\n",
      "0.242438130779\n",
      "0.242438127317\n",
      "0.242438130779\n",
      "0.231676518168\n",
      "0.23167651558\n",
      "0.231676518168\n",
      "0.224186755889\n",
      "0.22418675749\n",
      "0.224186755889\n",
      "0.220698848858\n",
      "0.220698848156\n",
      "0.220698848858\n",
      "0.219920088539\n",
      "0.219920088665\n",
      "0.219920088539\n",
      "0.219895357216\n",
      "0.219895357193\n",
      "0.219895357216\n",
      "0.219894447161\n",
      "0.21989444716\n",
      "0.219894447161\n",
      "0.219894444106\n",
      "0.219894444106\n",
      "0.219894444106\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n",
      "0.219894444059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -14.637374487654958\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([  8.88178420e-07,   1.42108547e-06])\n",
       "  message: 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 66\n",
       "      nit: 13\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 0.52089876,  2.05148951])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no initial input guess, make all 1's. \n",
    "res = minimize(line, np.array([1,1]), method='L-BFGS-B')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLikelihood that returns scatter (sigma).\n",
    "The above LogLikelihood function does not allow scatter to vary and return it as a parameter to be estiamted. This version will do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(estimate, true, n, sigma):\n",
    "    '''\n",
    "    LogLikelihood(estimate, true, n)\n",
    "    -- this function calculates the log-likelihood. To maximize \n",
    "        the likelihood you minimize: -1 * max(likelihood)\n",
    "\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    \n",
    "    estimate: float, is the estimate for y based on the model. \n",
    "    true:     float, is the y-data value. \n",
    "    n:        float, length of the estimate. \n",
    "    \n",
    "    RETURNS:\n",
    "    ----------\n",
    "    returns the log-likelihood. \n",
    "    \n",
    "    \n",
    "    NOTES:\n",
    "    ----------\n",
    "    The error is the true - estimate, or ydata - ymodel. These are also known \n",
    "    as the residuals of the fit; the distances between the actual data point \n",
    "    along the y-axis and the model. Errors/residuals are along y-axis only\n",
    "    meaning they are the vertical offsets and not perpendicular offsets. \n",
    "    Vertical offsets are the most common in linear regression. \n",
    "    \n",
    "    The likelihood function is set up for chi-squared as the statistic to minimize. \n",
    "        \n",
    "    It is in likelihood format:\n",
    "    L = ((1./(2*pi* (sig^2)))^(n/2)) * exp(- ( np.dot(error.T, error)/(2 * (sig^2))))\n",
    "\n",
    "    In log-likelihood format:\n",
    "    LogLike = -(n/2)*log(2*pi*(sig^2)) - (1/(2*(sig^2))) * np.dot(error.T, error)\n",
    "    \n",
    "    L is called the likelihood funtion. LogLike is the log of the likelihood function;\n",
    "    thus, LogLike = log(L).\n",
    "    \n",
    "    error is the error matrix:\n",
    "    (Y - X * theta).T  (Y-X * theta)\n",
    "    \n",
    "    .T stands for the transpose \n",
    "    \n",
    "    '''\n",
    "    error       = true - estimate   # ydata - yModel\n",
    "    L = ((1.0/(2.0*pi*sigma*sigma))**(n/2.)) * exp(-1*((np.dot(error.T, error))/(2.*sigma*sigma)))\n",
    "    return log(L) # log-likelihood.\n",
    "\n",
    "    \n",
    "def line(parameters):\n",
    "    '''\n",
    "    line(parameters)\n",
    "    -- Linear model. This is the function that will be minimized \n",
    "        by the LogLikelihood function. \n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    parameters: list, [m, b], where m and b are the slope and y-intercept. \n",
    "        \n",
    "    '''\n",
    "    m,b,sigma = parameters # m:slope, b:yintercept\n",
    "    yModel    = m * x + b  # estimate of y based on model. \n",
    "    f         = LogLikelihood(yModel, y, len(yModel), sigma)\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with approximate estimates for the parameters as inputs\n",
    "res = minimize(line, np.array([0.5, 2, 0.3]), method='L-BFGS-B')\n",
    "res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with no initial input guess.\n",
    "res = minimize(line, np.array([1,1,1]), method='L-BFGS-B')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The third parameter in res.x is the 1$\\sigma$ scatter about the linear relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcLogLikelihood(estimate, true, n, sigma):\n",
    "    '''\n",
    "    This is already set up for the chi-squared as the statistic to minimize.\n",
    "    \n",
    "    It is in likelihood format:\n",
    "    f = ((1./(2*pi* (sig^2)))^(n/2)) * exp(- ( np.dot(error.T, error)/(2 * (sig^2))))\n",
    "    f is the likelihood funtion\n",
    "    loglike = log(f)\n",
    "    \n",
    "    In log-likelihood format, this would be:\n",
    "    \n",
    "    l(theta) = -(n/2)*log(2*pi*(sig^2)) - (1/(2*(sig^2))) * np.dot(error.T, error)\n",
    "    where error is the error matrix:\n",
    "    (Y - X * theta).T (Y-X * theta)\n",
    "    \n",
    "    '''\n",
    "    error = true - estimate   # ydata - yModel, aka residual\n",
    "    f = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return f\n",
    "    \n",
    "#     f           = ((1.0/(2.0*pi*sigma*sigma))**(n/2))* \\\n",
    "#                     exp(-1*((np.dot(error.T, error))/(2*sigma*sigma)))\n",
    "#     return np.log(f)\n",
    "\n",
    "    \n",
    "def line(parameters):\n",
    "    '''\n",
    "    \n",
    "    Function to be minimized. This is a linear model. \n",
    "    \n",
    "    '''\n",
    "    m,b,sigma = parameters # m-slope, b-yintercept, scatter\n",
    "    yModel    = m * x + b  # estimate of y based on model. \n",
    "    f         = calcLogLikelihood(yModel, y, len(yModel), sigma)\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([0.52, 2, 0.3]), method='L-BFGS-B'); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([1,1,1]), method='L-BFGS-B'); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above can be combined into one function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(parameters):\n",
    "    '''\n",
    "    LogLikelihood(parameters)\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    parameters:  list of floats containing m, b, and sigma. m is slope, b in yintercept, \n",
    "                 sigma is the scatter about the fit. In our case, we want this returned. \n",
    "                 We leave it as a free variable. \n",
    "                 \n",
    "    NOTES:\n",
    "    ----------\n",
    "    Take the log of the likelihood first, and then return that as f. \n",
    "    \n",
    "    '''\n",
    "    m, b        = parameters\n",
    "    ymodel      = m * x + b    # linear model\n",
    "    n           = len(ymodel)\n",
    "    error       = y - ymodel\n",
    "    sigma       = np.std(error)\n",
    "    f           = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([0.52,2]), method='L-BFGS-B'); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([1,1]), method='L-BFGS-B'); res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We really should be passing the x and y data to the function, instead of assuming the function will read the global x and y variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(parameters, x, y):\n",
    "    '''\n",
    "    LogLikelihood(parameters)\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    parameters:  list of floats containing m, b, and sigma. m is slope, b in yintercept, \n",
    "                 sigma is the scatter about the fit. In our case, we want this returned. \n",
    "                 We leave it as a free variable. \n",
    "                 \n",
    "    NOTES:\n",
    "    ----------\n",
    "    Take the log of the likelihood first, and then return that as f. \n",
    "    \n",
    "    '''\n",
    "    m, b        = parameters\n",
    "    ymodel      = m * x + b    # linear model\n",
    "    n           = len(ymodel)\n",
    "    error       = y - ymodel\n",
    "    sigma       = np.std(error)\n",
    "    f           = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('/Users/KimiZ/GRBs2/Sample/AmatiDataSample.txt', sep=',', header=0)\n",
    "xdata = np.log10(data.eiso/(1.0E52))\n",
    "ydata = np.log10(data.epeakRest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([0.52,2]), method='L-BFGS-B', args=(xdata, ydata)); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(LogLikelihood, np.array([1,1]), method='L-BFGS-B', args=(xdata, ydata)); res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   perform least squares fit using scikitlearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
    "    ('linear', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "model = model.fit(x[:, np.newaxis], y)\n",
    "coefs = model.named_steps['linear'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
