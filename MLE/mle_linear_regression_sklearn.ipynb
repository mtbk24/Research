{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 27 02:46:12 2017\n",
    "\n",
    "@author: KimiZ\n",
    "\n",
    "\n",
    "Maximum Likelihood Estimation Linear Regression\n",
    "\n",
    "Sources:\n",
    "   http://jekel.me/2016/Maximum-Likelihood-Linear-Regression/\n",
    "   http://suriyadeepan.github.io/2017-01-22-mle-linear-regression/\n",
    "\n",
    "   Data analysis recipes: Fitting a model to dataâˆ—  \n",
    "      David W. Hogg, Jo Bovy, Dustin Lang (2010)\n",
    "        Equations 9-11\n",
    "          https://arxiv.org/pdf/1008.4686.pdf\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "import numpy as np\n",
    "from numpy import std, exp, log, log10\n",
    "\n",
    "from Zoldak.Math.loggingdata import log_margin_of_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.read_csv('/Users/KimiZ/GRBs2/Sample/AmatiDataSample.txt', sep=',', header=0)\n",
    "xoffset = 1.0E52\n",
    "xdata,xerrL,xerrU = log_margin_of_error(df.eiso/xoffset, \n",
    "                                        df.eiso_err_low/xoffset, \n",
    "                                        df.eiso_err_up/xoffset)\n",
    "\n",
    "ydata,yerrL,yerrU = log_margin_of_error(df.epeakRest, \n",
    "                                        df.epeakRest_err_low, \n",
    "                                        df.epeakRest_err_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   perform least squares fit using scikitlearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('linear', LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model   = Pipeline([('linear', LinearRegression(fit_intercept=False, normalize=False))])\n",
    "\n",
    "#model.fit(xdata[:, np.newaxis], ydata[:, np.newaxis])\n",
    "model.fit(xdata[:, np.newaxis],  ydata[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.74814495]]\n",
      "[ 271.34132385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KimiZ/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function residues_ is deprecated; ``residues_`` is deprecated and will be removed in 0.19\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "coefs     = model.named_steps['linear'].coef_\n",
    "print coefs\n",
    "\n",
    "residuals = model.named_steps['linear'].residues_\n",
    "print residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "linmod = linear_model.LinearRegression(fit_intercept=True, \n",
    "                                       normalize=False, \n",
    "                                       copy_X=True, n_jobs=None) #[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model3   = model2.fit(x[:, np.newaxis], y[:, np.newaxis])\n",
    "\n",
    "coefs = model3.named_steps['linear'].coef_\n",
    "residuals = model3.named_steps['linear'].residues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "linmod = linear_model.LinearRegression(fit_intercept=True, \n",
    "                                       normalize=False, \n",
    "                                       copy_X=True, n_jobs=None) #[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.linear_model.LinearRegression(fit_intercept=True, \n",
    "                                      normalize=False, \n",
    "                                      copy_X=True, n_jobs=None)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([('poly',   PolynomialFeatures(degree=2)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model.fit(x[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KimiZ/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function residues_ is deprecated; ``residues_`` is deprecated and will be removed in 0.19\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#coefs = model.named_steps['linear'].coef_\n",
    "\n",
    "#   perform least squares fit using scikitlearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model2   = Pipeline([('linear', LinearRegression(fit_intercept=True, normalize=True, n_jobs=3))])\n",
    "model3   = model2.fit(xdata[:, np.newaxis], ydata[:, np.newaxis])\n",
    "coefs    = model3.named_steps['linear'].coef_\n",
    "residuals = model3.named_steps['linear'].residues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52089877]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "  Now that we have a function that works, we will use it in conjunction with a MCMC technique for generating data.\n",
    "  We do this to get uncertainty on our slope, y-intercept, and sigma scatter.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import emcee\n",
    "import corner\n",
    "#from Amati.tools import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   AMATI DATA.\n",
    "data = None\n",
    "File            = '/Users/KimiZ/GRBs2/analysis/Sample/AmatiDataSample.txt'\n",
    "data            = pd.read_csv(File , sep=',', header=0)\n",
    "\n",
    "xdata           = np.log10(data.eiso/(1.0E52))\n",
    "ydata           = np.log10(data.epeak)\n",
    "xdataerr        = np.log10(data.eiso_err/(1.0E52))\n",
    "ydataerr        = np.log10(data.epeak_err)\n",
    "\n",
    "\n",
    "\n",
    "# ANOTHER VERSION OF THE SAME, BUT WRITTEN SLIGHTLY DIFFERENT.\n",
    "# THIS TIME WITH THE DOT PRODUCT AND MATRIX.\n",
    "def LogLikelihood(parameters, x, y):\n",
    "    m, b, sigma       = parameters\n",
    "    ymodel            = m * x + b\n",
    "    n                 = float(len(ymodel))\n",
    "    error             = y - ymodel  # true y data - model of a line (y - mx - b) or (y - (mx+b))\n",
    "    L   = ((n/2.) * np.log(2.*np.pi*sigma**2) + 1./(2.*sigma**2)* np.dot(error.T,error))\n",
    "    # SINCE WE LEFT OFF THE (-) SIGN IN THE EQUATION ABOVE FOR L, WE NEED TO DROP IT \n",
    "    # IN THE RETURN AS WELL. \n",
    "    return L\n",
    "\n",
    "#result = minimize(LogLikelihood, np.array([0.52,2,0.3]), method='L-BFGS-B', args=(x, y)); res\n",
    "\n",
    "x             = xdata\n",
    "y             = ydata\n",
    "\n",
    "result = minimize(LogLikelihood, np.array([1,1,1]), method='L-BFGS-B', args=(x, y)); result\n",
    "'''\n",
    "In [3]: result\n",
    "Out[3]: \n",
    "      fun: -14.637374487654924\n",
    " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
    "      jac: array([  5.68434189e-06,   1.42108547e-06,   1.42108547e-06])\n",
    "  message: 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
    "     nfev: 112\n",
    "      nit: 16\n",
    "   status: 0\n",
    "  success: True\n",
    "        x: array([ 0.52089876,  2.05148951,  0.21989444])\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# SAVE THE PARAMETERS FOUND BY MAXIMUM LIKELIHOOD. WILL USE THESE TO GENERATE DATA BASED\n",
    "# OFF OF y = mx + b, WHERE m = m_ml AND b = b_ml, WITH SCATTER f_ml\n",
    "m_ml, b_ml, f_ml  = result[\"x\"]  # or result.x\n",
    "\n",
    "\n",
    "# PLOT THE DATA AND MODEL\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, result['x'][0] * x + result['x'][1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def Prior(parameters):\n",
    "    m, b, f = parameters\n",
    "    #if 0.4 < m < 0.6 and 1.0 < b < 3.0 and 0.1 < f < 0.3:\n",
    "    #if 0.1 < m < 1.0 and 0.1 < b < 5.0 and 0.1 < f < 1.0:\n",
    "    if -1.0 < m < 1.0 and 0 < b < 10.0 and 0 < f < 1.0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def Prob(parameters, x, y): \n",
    "    lp = Prior(parameters)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp - LogLikelihood(parameters, x, y) # lp + LogLikelihood(parameters, x, y) # our version needs - sign.\n",
    "\n",
    "\n",
    "ndim, nwalkers = 3, 100\n",
    "pos = [result.x + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, Prob, args=(x, y))\n",
    "sampler.run_mcmc(pos, 500)\n",
    "samples = sampler.chain[:, 50:, :].reshape((-1, ndim))\n",
    "\n",
    "\n",
    "# WE TAKE THE MAXIMUM LIKELIHOOD RETURNED VALUES TO BE THE TRUE ONES.\n",
    "m_true, b_true, f_true = result.x\n",
    "\n",
    "#samples[:, 2] = np.exp(samples[:, 2])  # our f parameter isn't logged.\n",
    "m_mcmc, b_mcmc, f_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "\n",
    "m_mcmc, b_mcmc, f_mcmc\n",
    "\n",
    "\n",
    "\n",
    "print('%.3f %.3f  %.3f'%(m_mcmc[0], m_mcmc[1], m_mcmc[2]))\n",
    "print('%.3f %.3f  %.3f'%(b_mcmc[0], b_mcmc[1], b_mcmc[2]))\n",
    "print('%.3f %.3f  %.3f'%(f_mcmc[0], f_mcmc[1], f_mcmc[2]))\n",
    "\n",
    "print('%.3f %.3f'%(m_mcmc[0], m_ml))\n",
    "print('%.3f %.3f'%(b_mcmc[0], b_ml))\n",
    "print('%.2f %.2f'%(f_mcmc[0], f_ml))\n",
    "\n",
    "m_err = (m_mcmc[1] + m_mcmc[2])/(2.0)\n",
    "b_err = (b_mcmc[1] + b_mcmc[2])/(2.0)\n",
    "f_err = (f_mcmc[1] + f_mcmc[2])/(2.0)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n')\n",
    "print('log(Epk) = b + m log(Eiso) \\n\\n b = %.3f (+- %.3f) \\n m = %.3f (+- %.3f) \\n f = %.2f (+- %.2f)'%(b_true,b_err,m_true,m_err,f_true,f_err))\n",
    "print('  f is the extrinsic scatter and sigma in the parameters. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    PLOTS\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "props = dict(boxstyle=None, facecolor='gainsboro', \n",
    "             linewidth=0.0, alpha=0.57)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "xl = np.array([-10, 10])\n",
    "for m, b, f in samples[np.random.randint(len(samples), size=100)]:\n",
    "    plt.plot(xl, m * xl + b, color=\"k\", alpha=0.1)\n",
    "plt.plot(xl, m_true * xl + b_true, color=\"r\", lw=2, alpha=0.8)\n",
    "plt.errorbar(x, y, xerr=xdataerr, yerr=ydataerr, fmt=\".k\", \n",
    "             capsize=0, alpha=0.2)\n",
    "\n",
    "#plt.plot(x, 0.9 * x + b_true, color='green')\n",
    "plt.plot(x, y, \".\")\n",
    "plt.figtext(0.15,0.8,'$%s=%.3f + %.3f %s$'%('\\log E^*_{pk}',\n",
    "                                  b_true, m_true,\n",
    "                                  ' \\ \\log E_{iso}'), \n",
    "                                    fontsize=18,\n",
    "                                     bbox=props)                                 \n",
    "plt.ylim(0,5)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylabel('$E^{*}_{pk}$ ($keV$)', fontsize=18)\n",
    "plt.xlabel('$E_{iso}$ ($erg$)', fontsize=18)\n",
    "plt.show()\n",
    "#fig.tight_layout(pad=0.25, w_pad=0, h_pad=0)\n",
    "#plt.savefig(\"/Users/KimiZ/GRBs2/python_modeling/plot_Amati_fullsample.png\", dpi=250)\n",
    "\n",
    "#%%\n",
    "\n",
    "print('%.3f %.3f  %.3f'%(m_mcmc[0], m_mcmc[1], m_mcmc[2]))\n",
    "print('%.3f %.3f  %.3f'%(b_mcmc[0], b_mcmc[1], b_mcmc[2]))\n",
    "print('%.3f %.3f  %.3f'%(f_mcmc[0], f_mcmc[1], f_mcmc[2]))\n",
    "\n",
    "m_err = (m_mcmc[1] + m_mcmc[2])/(2.0)\n",
    "b_err = (b_mcmc[1] + b_mcmc[2])/(2.0)\n",
    "f_err = (f_mcmc[1] + f_mcmc[2])/(2.0)\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle=None, facecolor='gainsboro', linewidth=0.0, alpha=0.57)\n",
    "            \n",
    "label_dict = dict(fontsize=20)\n",
    "fig = corner.corner(samples, labels=[\"$m$\", \"$b$\", \"$\\sigma_{ext}$\"], \n",
    "                    truth_color='red', \n",
    "                    label_kwargs=label_dict,\n",
    "                    truths=[m_true, b_true, f_true])\n",
    "                    \n",
    "plt.figtext(0.5,0.71,'$\\log E^*_{pk} \\ =\\ b + m \\ \\log E_{iso}$ \\n\\n $b \\ =%.3f(\\pm%.3f)$ \\n $m=%.3f(\\pm%.3f)$ \\n $\\sigma_{ext}=%.2f (\\pm%.2f)$'%(b_true,b_err,m_true,m_err,f_true,f_err),\n",
    "                                    fontsize = 20,\n",
    "                                     bbox=props)                                 \n",
    "                                     \n",
    "#fig.savefig(\"/Users/KimiZ/GRBs2/python_modeling/cornerplot_Amati_fullsample.png\", dpi=250)\n",
    "\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('/Users/KimiZ/GRBs2/Sample/AmatiDataSample.txt', sep=',', header=0)\n",
    "x     = np.log10(data.eiso/(1.0E52))\n",
    "y     = np.log10(data.epeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
