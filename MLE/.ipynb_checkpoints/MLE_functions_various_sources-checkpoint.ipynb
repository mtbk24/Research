{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import numpy as np\n",
    "from numpy import std, exp, log, log10\n",
    "\n",
    "#from Zoldak.Math.loggingdata import log_margin_of_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MINE\n",
    "\n",
    "def LogLikelihood(estimate, true, n, sigma):\n",
    "    '''\n",
    "    LogLikelihood(estimate, true, n, sigma)\n",
    "   \n",
    "    The scipy.optimize.minimize module minimizes functions.\n",
    "    We want to maximize the likelihood function. \n",
    "    To maximize the likelihood, you minimize the negative log-likelihood,\n",
    "    hence the -1 * LogLikelihood.\n",
    "    \n",
    "    This is the LogLikelihood that can then be multiplied by -1 and minimized.\n",
    "    \n",
    "\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    \n",
    "    estimate: float, is the estimate for y based on the model. \n",
    "    true:     float, is the y-data value. \n",
    "    n:        float, length of the estimate. \n",
    "    sigma:    float, scatter about the relation. \n",
    "    \n",
    "    if you don't want sigma to be a free parameter, use\n",
    "        sigma = np.std(error) \n",
    "    and remove it from parameters assignment. \n",
    "    \n",
    "    RETURNS:\n",
    "    ----------\n",
    "    returns the log-likelihood. \n",
    "    \n",
    "    \n",
    "    NOTES:\n",
    "    ----------\n",
    "    The error is the true - estimate, or ydata - ymodel. These are also known \n",
    "    as the residuals of the fit; the distances between the actual data point \n",
    "    along the y-axis and the model. Errors/residuals are along y-axis only\n",
    "    meaning they are the vertical offsets and not perpendicular offsets. \n",
    "    Vertical offsets are the most common in linear regression. \n",
    "    \n",
    "    '''\n",
    "    error = true - estimate       # ydata - yModel, aka residual\n",
    "    L = (-0.5*n*log(2.0*pi*sigma*sigma)) - (sum(error**2)/(2.0*sigma*sigma))\n",
    "    return L\n",
    "    \n",
    "def linear_model(parameters, x, y):\n",
    "    '''\n",
    "    linear_model(parameters, x, y)\n",
    "    -- Linear model. \n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------\n",
    "    parameters: list, [m, b, sigma], where m and b are the slope and y-intercept\n",
    "                and sigma is the scatter about the lienar relation. \n",
    "    x:          list or array of xdata. **\n",
    "    y:          list or array of ydata. **\n",
    "    \n",
    "    ** - pandas dataframe Series work as well. \n",
    "    \n",
    "    RETURNS:\n",
    "    ----------\n",
    "    returns -1 * Log-Likelihood, where the yModel is a line. \n",
    "    \n",
    "        \n",
    "    '''\n",
    "    m,b,sigma = parameters   # m:slope, b:yintercept\n",
    "    yModel    = m * x + b    # estimate of y based on model. \n",
    "    return (-1 * LogLikelihood(yModel, y, len(yModel), sigma) ) # -1 * loglike\n",
    "\n",
    "# run the calculations\n",
    "result = minimize( fun     = linear_model,       # function to minimize (linear model)\n",
    "                   x0      = np.array([1,1,1]),  # initial guess for all parameters. \n",
    "                   method  = 'L-BFGS-B',         # method of solver. \n",
    "                   args    = (xdata, ydata))     # extra arguments to pass to the function. \n",
    "                                                 # Ours are x an y values.  \n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   define a function to calculate the log likelihood\n",
    "def calcLogLikelihood(guess, true, n):\n",
    "    error = true-guess\n",
    "    sigma = np.std(error)\n",
    "    f = ((1.0/(2.0*math.pi*sigma*sigma))**(n/2))* \\\n",
    "        np.exp(-1*((np.dot(error.T,error))/(2*sigma*sigma)))\n",
    "    return np.log(f)\n",
    "\n",
    "#   define my function which will return the objective function to be minimized\n",
    "def myFunction(var):\n",
    "    #   load my  data\n",
    "    [x, y] = np.load('myData.npy')\n",
    "    yGuess = (var[2]*(x**2)) + (var[1]*x) + var[0]\n",
    "    f = calcLogLikelihood(yGuess, y, float(len(yGuess)))\n",
    "    return (-1*f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://dfm.io/emcee/current/user/line/\n",
    "\n",
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b, lnf = theta\n",
    "    model = m * x + b\n",
    "    inv_sigma2 = 1.0/(yerr**2 + model**2*np.exp(2*lnf))\n",
    "    return -0.5*(np.sum((y-model)**2*inv_sigma2 - np.log(inv_sigma2)))\n",
    "\n",
    "nll = lambda *args: -lnlike(*args)\n",
    "\n",
    "result = op.minimize(nll, [m_true, b_true, np.log(f_true)], args=(x, y, yerr))\n",
    "m_ml, b_ml, lnf_ml = result[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ef52b618c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# let’s start with some random coefficient guesses and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLERegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Nelder-Mead'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##options={'disp': True})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f\n",
    "def MLERegression(params, x, y):\n",
    "    intercept, beta, sd = params[0], params[1], params[2] # inputs are guesses at our parameters\n",
    "    yhat = intercept + beta*x # predictions\n",
    "    # compute PDF of observed values normally distributed around mean (yhat)\n",
    "    # with a standard deviation of sd\n",
    "    negLL = -np.sum( stats.norm.logpdf(y, loc=yhat, scale=sd) )\n",
    "    return(negLL) # return negative LL\n",
    "\n",
    "# let’s start with some random coefficient guesses and optimize\n",
    "\n",
    "guess = np.array([1,1,1])\n",
    "results = minimize(MLERegression, guess, method = 'Nelder-Mead', args=(xdata,ydata)) ##options={'disp': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
